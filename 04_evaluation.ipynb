{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パフォーマンスと品質評価ツールの使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import GroundednessProEvaluator, GroundednessEvaluator, RetrievalEvaluator, RelevanceEvaluator\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "#credential = DefaultAzureCredential()\n",
    "\n",
    "# For Groundedness Evaluator\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_MODEL\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "# For Groundedness Pro Evaluator\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "tenant_id = os.environ.get(\"TENANT_ID\")\n",
    "credential = InteractiveBrowserCredential(tenant_id=tenant_id) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./answer_sample.md\", \"r\", encoding = 'utf-8') as f:\n",
    "    response = f.read()\n",
    "with open(\"./context_sample.txt\", \"r\", encoding = 'utf-8') as f:\n",
    "    context = f.read()\n",
    "with open(\"./ground_truth.md\", \"r\", encoding = 'utf-8') as f:\n",
    "    ground_truth = f.read()\n",
    "query = \"PCS警告灯が点滅または点灯する場合の対処法\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroundednessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzing Groundedness evaluator\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "\n",
    "query_response = dict(query=query, context=context, response=response)\n",
    "# Running Groundedness Evaluator on a query and response pair\n",
    "groundedness_score = groundedness_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrievalEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_eval = RetrievalEvaluator(model_config)\n",
    "query_response = dict(query=query, context=context)\n",
    "\n",
    "relevance_score = retrieval_eval(**query_response)\n",
    "print(relevance_score)\n",
    "relevance_score[\"retrieval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RelevanceEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "query_response = dict(query=query, response=context)\n",
    "\n",
    "relevance_score = relevance_eval(**query_response)\n",
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[Optional]** Groundedness Pro evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzing Groundedness Pro evaluator\n",
    "# Supported regions are ueaastus2 and sweedencentral\n",
    "groundedness_pro_eval = GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n",
    "query_response = dict(query=query, context=context, response=response)\n",
    "\n",
    "groundedness_pro_score = groundedness_pro_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(groundedness_pro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BleuScoreEvaluator\n",
    "BLEU score measures the similarity by shared n-grams between the generated text and the ground truth, focusing more on precision and indirectly on recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "bleu = BleuScoreEvaluator()\n",
    "\n",
    "query_response = dict(response=response, ground_truth=ground_truth)\n",
    "result = bleu(\n",
    "    **query_response\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GleuScoreEvaluator\n",
    "GLEU score measures the similarity by shared n-grams between the generated text and ground truth, similar to the BLEU score, focusing on both precision and recall. It addresses the drawbacks of the BLEU score using a per-sentence reward objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "gleu = GleuScoreEvaluator()\n",
    "\n",
    "query_response = dict(response=response, ground_truth=ground_truth)\n",
    "\n",
    "result = gleu(\n",
    "    **query_response\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeteorScoreEvaluator\n",
    "METEOR score measures the similarity by shared n-grams between the generated text and the ground truth, similar to the BLEU score, focusing on precision and recall. It addresses limitations of other metrics like the BLEU score by considering synonyms, stemming, and paraphrasing for content alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "meteor = MeteorScoreEvaluator(alpha=0.9, beta=3.0, gamma=0.5)\n",
    "\n",
    "query_response = dict(response=response, ground_truth=ground_truth)\n",
    "\n",
    "result = meteor(\n",
    "    **query_response\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RougeScoreEvaluator\n",
    "ROUGE measures the similarity by shared n-grams between the generated text and the ground truth. <br>\n",
    "ROUGE precision reflects the fraction of the n-grams in the response that are also in the ground truth. <br>\n",
    "ROUGE recall is the fraction of n-grams in ground truth that also appear in the response. <br>\n",
    "ROUGE f1_score is calculated from ROUGE precision and ROUGE recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)\n",
    "\n",
    "query_response = dict(response=response, ground_truth=ground_truth)\n",
    "\n",
    "result = rouge(\n",
    "    **query_response\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[Optional]** Tracking the evaluation results in Azure AI Foundry project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"query_response.jsonl\",\n",
    "    evaluators={\n",
    "        \"bleu\": bleu,\n",
    "        \"gleu\": gleu,\n",
    "        \"meteor\": meteor,\n",
    "        \"rouge\": rouge,\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=azure_ai_project,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
