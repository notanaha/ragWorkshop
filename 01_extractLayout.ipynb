{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77880dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04173460",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv(\"FR_ENDPOINT\")\n",
    "key = os.getenv(\"FR_KEY\")\n",
    "\n",
    "# Instantiate DocumentAnalysisClient\n",
    "document_analysis_client = DocumentIntelligenceClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d093e",
   "metadata": {},
   "source": [
    "<h3>Document Intelligence - Text Extraction by Layout Model\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe62824",
   "metadata": {},
   "source": [
    "\n",
    "├── pdf_dir  \n",
    "│&emsp; &emsp; ├── text_dir  \n",
    "│&emsp; &emsp; └── pdf files  \n",
    "this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac763e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name='o200k_base',\n",
    "    chunk_size=4000, \n",
    "    chunk_overlap=500\n",
    ")\n",
    "\n",
    "#テキストファイルを読み込んで、指定のトークン数のチャンクファイルに分割します。\n",
    "def splitChunkFile(filepath):\n",
    "    dirname = os.path.dirname(filepath)\n",
    "    output_path = dirname + \"/chunks/\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    f = open(filepath, 'r', encoding='UTF-8')\n",
    "    data = f.read()\n",
    "    chunk = text_splitter.split_text(data)\n",
    "\n",
    "    #chunk単位でループ\n",
    "    for i, chunkedtext in enumerate(chunk):        \n",
    "\n",
    "        basename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        outputfilepath = output_path + basename + \"-\" + str(i) + \".txt\"\n",
    "        \n",
    "        #print(i, len(enc.encode(chunkedtext)), outputfilepath)\n",
    "        with open(outputfilepath, 'w', encoding='UTF-8') as fo:\n",
    "            fo.write(chunkedtext)\n",
    "\n",
    "        fo.close()\n",
    "    f.close()\n",
    "   \n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5067951c",
   "metadata": {},
   "source": [
    "<h5>Form Recognizer - Layout Model によるテキストの抽出\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c75dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = Path('./pdf')\n",
    "text_dir = Path('./text')\n",
    "text_path = Path(os.path.join(pdf_dir, text_dir))\n",
    "os.makedirs(text_path, exist_ok=True)\n",
    "\n",
    "for pdf in next(os.walk(pdf_dir))[2]:\n",
    "\n",
    "    with open(os.path.join(pdf_dir, pdf), \"rb\") as f:        \n",
    "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\")\n",
    "        result = poller.result()\n",
    "        text = result.content.replace(\":unselected:\", \"\").replace(\":selected:\", \"\")\n",
    "\n",
    "        chunk = text_splitter.split_text(text)\n",
    "\n",
    "        #chunk単位でループ\n",
    "        for i, chunkedtext in enumerate(chunk):        \n",
    "\n",
    "            basename = os.path.splitext(os.path.basename(pdf))[0]\n",
    "            filename = basename + \"_\" + str(i) + \".txt\"\n",
    "            outputfilepath = os.path.join(text_path, filename)\n",
    "            \n",
    "            #print(i, len(enc.encode(chunkedtext)), outputfilepath)\n",
    "            with open(outputfilepath, 'w', encoding='UTF-8') as fo:\n",
    "                fo.write(chunkedtext)\n",
    "\n",
    "            fo.close()\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3986b",
   "metadata": {},
   "source": [
    "<h5>Form Recognizer - Layout Model によるテキストの抽出 (Mark Down)\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1665bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_markdown_headings(markdown_text):\n",
    "    # Convert \"===\" headers to \"#\"\n",
    "    markdown_text = re.sub(r'^(.*?)\\n={3,}$', r'# \\1', markdown_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Convert \"---\" headers to \"##\"\n",
    "    markdown_text = re.sub(r'^(.*?)\\n-{3,}$', r'## \\1', markdown_text, flags=re.MULTILINE)\n",
    "    \n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path('./pdf/serviceInfo.pdf')\n",
    "output_path = Path('./text')\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(input_path, \"rb\") as f:        \n",
    "    #data_bytes = f.read()\n",
    "    poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", output_content_format=ContentFormat.MARKDOWN)\n",
    "    result = poller.result()\n",
    "    text = convert_markdown_headings(result.content)\n",
    "    #text = result.content.replace('\\n',' \\n').replace(\":unselected:\", \" \")\n",
    "\n",
    "    out_fname = Path(input_path).stem + \".md\"\n",
    "    with open(os.path.join(output_path, out_fname), 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
