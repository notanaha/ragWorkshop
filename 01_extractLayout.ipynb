{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77880dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentContentFormat, AnalyzeOutputOption, AnalyzeResult\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04173460",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv(\"FR_ENDPOINT\")\n",
    "key = os.getenv(\"FR_KEY\")\n",
    "\n",
    "# Instantiate DocumentAnalysisClient\n",
    "document_analysis_client = DocumentIntelligenceClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d093e",
   "metadata": {},
   "source": [
    "<h3>Document Intelligence - Text Extraction by Layout Model\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe62824",
   "metadata": {},
   "source": [
    "\n",
    "├── pdf_dir  \n",
    "│&emsp; &emsp; ├── text_dir  \n",
    "│&emsp; &emsp; └── pdf files  \n",
    "this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac763e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name='o200k_base',\n",
    "    chunk_size=4000, \n",
    "    chunk_overlap=500\n",
    ")\n",
    "\n",
    "#テキストファイルを読み込んで、指定のトークン数のチャンクファイルに分割します。Load the text file and split it into chunk files with the specified number of tokens.\n",
    "#この関数は本ノートブック内で使用されていません。This function is not used within this notebook.\n",
    "def splitChunkFile(filepath):\n",
    "    dirname = os.path.dirname(filepath)\n",
    "    output_path = dirname + \"/chunks/\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    f = open(filepath, 'r', encoding='UTF-8')\n",
    "    data = f.read()\n",
    "    chunk = text_splitter.split_text(data)\n",
    "\n",
    "    #chunk単位でループ\n",
    "    for i, chunkedtext in enumerate(chunk):        \n",
    "\n",
    "        basename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        outputfilepath = output_path + basename + \"-\" + str(i) + \".txt\"\n",
    "        \n",
    "        #print(i, len(enc.encode(chunkedtext)), outputfilepath)\n",
    "        with open(outputfilepath, 'w', encoding='UTF-8') as fo:\n",
    "            fo.write(chunkedtext)\n",
    "\n",
    "        fo.close()\n",
    "    f.close()\n",
    "   \n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5067951c",
   "metadata": {},
   "source": [
    "<h5>Document Intelligence - Layout Model によるテキストの抽出\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c75dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = Path('./pdf')\n",
    "text_dir = Path('./text')\n",
    "text_path = Path(os.path.join(pdf_dir, text_dir))\n",
    "os.makedirs(text_path, exist_ok=True)\n",
    "\n",
    "for pdf in next(os.walk(pdf_dir))[2]:\n",
    "\n",
    "    with open(os.path.join(pdf_dir, pdf), \"rb\") as f:        \n",
    "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", body=f, content_type=\"application/octet-stream\")\n",
    "        result = poller.result()\n",
    "        text = result.content.replace(\":unselected:\", \"\").replace(\":selected:\", \"\")\n",
    "\n",
    "        chunk = text_splitter.split_text(text)\n",
    "\n",
    "        #chunk単位でループ\n",
    "        for i, chunkedtext in enumerate(chunk):        \n",
    "\n",
    "            basename = os.path.splitext(os.path.basename(pdf))[0]\n",
    "            filename = basename + \"_\" + str(i) + \".txt\"\n",
    "            outputfilepath = os.path.join(text_path, filename)\n",
    "            \n",
    "            #print(i, len(enc.encode(chunkedtext)), outputfilepath)\n",
    "            with open(outputfilepath, 'w', encoding='UTF-8') as fo:\n",
    "                fo.write(chunkedtext)\n",
    "\n",
    "            fo.close()\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd7d7e",
   "metadata": {},
   "source": [
    "### 以下は参考のため記載しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3986b",
   "metadata": {},
   "source": [
    "<h5>Document Intelligence - Layout Model によるテキストの抽出 (Mark Down)\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1665bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "\n",
    "# Unify the format of headings in markdown text\n",
    "def convert_markdown_headings(markdown_text):\n",
    "    # Convert \"===\" headers to \"#\"\n",
    "    markdown_text = re.sub(r'^(.*?)\\n={3,}$', r'# \\1', markdown_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Convert \"---\" headers to \"##\"\n",
    "    markdown_text = re.sub(r'^(.*?)\\n-{3,}$', r'## \\1', markdown_text, flags=re.MULTILINE)\n",
    "    \n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = Path('./pdf')\n",
    "text_dir = Path('./text2')\n",
    "text_path = Path(os.path.join(pdf_dir, text_dir))\n",
    "os.makedirs(text_path, exist_ok=True)\n",
    "\n",
    "for pdf in next(os.walk(pdf_dir))[2]:\n",
    "\n",
    "    with open(os.path.join(pdf_dir, pdf), \"rb\") as f:        \n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", body=f, content_type=\"application/octet-stream\", output_content_format=DocumentContentFormat.MARKDOWN \n",
    "        )\n",
    "        \n",
    "        result = poller.result()\n",
    "        md_content = result.content\n",
    "        md_content = convert_markdown_headings(result.content)\n",
    "\n",
    "        title = Path(pdf).stem\n",
    "        out_fname = title + \".md\"\n",
    "        with open(os.path.join(text_path, out_fname), 'w', encoding='utf-8') as outfile:\n",
    "            #outfile.write(text)\n",
    "            outfile.write(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c1719",
   "metadata": {},
   "source": [
    "### Chunk Markdown documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "# Split the document into chunks base on markdown headers.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "#    (\"##\", \"Header 2\"),\n",
    "#    (\"###\", \"Header 3\"),\n",
    "]\n",
    "# Include the headers in the splits.\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = Path('./pdf')\n",
    "text_dir = Path('./text2')\n",
    "chunks_dir = Path('./chunks')\n",
    "text_path = Path(os.path.join(pdf_dir, text_dir))\n",
    "chunks_path = Path(os.path.join(pdf_dir, chunks_dir))\n",
    "os.makedirs(chunks_path, exist_ok=True)\n",
    "\n",
    "for text in next(os.walk(text_path))[2]:\n",
    "\n",
    "    with open(os.path.join(text_path, text), 'r', encoding=\"utf-8\") as f:        \n",
    "        content = f.read()\n",
    "        splits = text_splitter.split_text(content)\n",
    "\n",
    "        for i, split in enumerate(splits):\n",
    "            title = Path(text).stem\n",
    "            out_fname = title + f\"_{i}.md\"\n",
    "            with open(os.path.join(chunks_path, out_fname), 'w', encoding='utf-8') as outfile:\n",
    "                outfile.write(split.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba772da8",
   "metadata": {},
   "source": [
    "### 図の取り出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839503ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample_document=\"./Large Language Models A Survey‾02.jpeg\"\n",
    "with open(path_to_sample_document, \"rb\") as f:\n",
    "    poller = document_analysis_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\",\n",
    "        body=f,\n",
    "        output=[AnalyzeOutputOption.FIGURES],\n",
    "        content_type=\"application/octet-stream\",\n",
    "    )\n",
    "result: AnalyzeResult = poller.result()\n",
    "operation_id = poller.details[\"operation_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.figures:\n",
    "    for figure in result.figures:\n",
    "        if figure.id:\n",
    "            response = document_analysis_client.get_analyze_result_figure(\n",
    "                model_id=result.model_id, result_id=operation_id, figure_id=figure.id\n",
    "            )\n",
    "            with open(f\"Large Language Models A Survey‾{figure.id}.png\", \"wb\") as writer:\n",
    "                writer.writelines(response)\n",
    "else:\n",
    "    print(\"No figures found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
